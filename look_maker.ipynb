{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lib.hair import HairMaskPipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import webcolors\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hair_color(color_name, input_image_path):\n",
    "  hair_mask_pipeline = HairMaskPipeline()\n",
    "\n",
    "  hair_mask_path = hair_mask_pipeline.generate_hair_mask(image_path=input_image_path, output_mask_path='output/masks/hair_mask.png')\n",
    "\n",
    "  print('Hair mask generated at', hair_mask_path)\n",
    "  # Read the input image and hair mask using cv2\n",
    "  input_image_cv = cv2.imread(input_image_path, cv2.IMREAD_UNCHANGED)\n",
    "  hair_mask_cv = cv2.imread(hair_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Add an alpha channel to the input image if it doesn't have one\n",
    "  if input_image_cv.shape[2] == 3:\n",
    "    input_image_cv = cv2.cvtColor(input_image_cv, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "  rgb_value = webcolors.name_to_rgb(color_name)\n",
    "\n",
    "  print('RGB value for', color_name, 'is', rgb_value)\n",
    "  print('RGB value for', color_name, 'is', rgb_value.red, rgb_value.green, rgb_value.blue)\n",
    "\n",
    "  # Create a red layer with the same size as the input image\n",
    "  color_layer_cv = np.zeros_like(input_image_cv)\n",
    "  color_layer_cv[:, :, 0] = rgb_value.blue  # Set the blue channel\n",
    "  color_layer_cv[:, :, 1] = rgb_value.green  # Set the green channel\n",
    "  color_layer_cv[:, :, 2] = rgb_value.red  # Set the red channel\n",
    "\n",
    "  # Apply the red color only to the mask area\n",
    "  color_layer_cv[:, :, 3] = hair_mask_cv\n",
    "\n",
    "  # Composite the red layer onto the input image using the mask with 10% alpha\n",
    "  colored_image_cv = input_image_cv.copy()\n",
    "  alpha = 0.15\n",
    "  colored_image_cv[hair_mask_cv > 0] = cv2.addWeighted(input_image_cv[hair_mask_cv > 0], 1 - alpha, color_layer_cv[hair_mask_cv > 0], alpha, 0)\n",
    "\n",
    "  # Save the result\n",
    "  output_path = 'output/colored_hair.png'\n",
    "  cv2.imwrite(output_path, colored_image_cv)\n",
    "\n",
    "  print(f\"Image saved at Output path: {output_path}\")\n",
    "  return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_hair_color('blue', 'source_data/stefan_original_photo.jpg')\n",
    "\n",
    "def hair_transform(selected_color, image_path):\n",
    "    return set_hair_color(selected_color, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d101e880699348f398c5048d3a23e083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Color:', options=('blue', 'red', 'green', 'yellow', 'purple', 'pink', 'orange', 'brown',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3461d9dc41441368c18bb214f0a0ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='source_data/stefan_original_photo.jpg', description='Image Path:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3227dd259a44f3d927a9e6f12c83a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Apply Hair Color', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26693528350d4dbcaf8825b143e3a4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarja\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c6c4219f894b0e8d5aa43cf33cec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14591661aef543fcb1a38742ec4929f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for gray is IntegerRGB(red=128, green=128, blue=128)\n",
      "RGB value for gray is 128 128 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19c9b4cd30446089cbde3771901ea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Image not found at source_data/Augustus.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mon_apply_button_clicked\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_apply_button_clicked\u001b[39m(b):\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mhair_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_dropdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mhair_transform\u001b[1;34m(selected_color, image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhair_transform\u001b[39m(selected_color, image_path):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mset_hair_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mset_hair_color\u001b[1;34m(color_name, input_image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_hair_color\u001b[39m(color_name, input_image_path):\n\u001b[0;32m      2\u001b[0m   hair_mask_pipeline \u001b[38;5;241m=\u001b[39m HairMaskPipeline()\n\u001b[1;32m----> 4\u001b[0m   hair_mask_path \u001b[38;5;241m=\u001b[39m \u001b[43mhair_mask_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_hair_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mask_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/masks/hair_mask.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHair mask generated at\u001b[39m\u001b[38;5;124m'\u001b[39m, hair_mask_path)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;66;03m# Read the input image and hair mask using cv2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarja\\OneDrive - Hearst\\Documents\\AI&ML\\Code\\Project-3\\lib\\hair.py:57\u001b[0m, in \u001b[0;36mHairMaskPipeline.generate_hair_mask\u001b[1;34m(self, image_path, output_mask_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam_predictor\u001b[38;5;241m.\u001b[39mset_image(image_rgb)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Image not found at source_data/Augustus.jpg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b36e7489ee64e878ca0ac5e2bef7391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Image not found at source_data/Augustus_Crop.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mon_apply_button_clicked\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_apply_button_clicked\u001b[39m(b):\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mhair_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_dropdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mhair_transform\u001b[1;34m(selected_color, image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhair_transform\u001b[39m(selected_color, image_path):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mset_hair_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mset_hair_color\u001b[1;34m(color_name, input_image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_hair_color\u001b[39m(color_name, input_image_path):\n\u001b[0;32m      2\u001b[0m   hair_mask_pipeline \u001b[38;5;241m=\u001b[39m HairMaskPipeline()\n\u001b[1;32m----> 4\u001b[0m   hair_mask_path \u001b[38;5;241m=\u001b[39m \u001b[43mhair_mask_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_hair_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mask_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/masks/hair_mask.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHair mask generated at\u001b[39m\u001b[38;5;124m'\u001b[39m, hair_mask_path)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;66;03m# Read the input image and hair mask using cv2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarja\\OneDrive - Hearst\\Documents\\AI&ML\\Code\\Project-3\\lib\\hair.py:57\u001b[0m, in \u001b[0;36mHairMaskPipeline.generate_hair_mask\u001b[1;34m(self, image_path, output_mask_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam_predictor\u001b[38;5;241m.\u001b[39mset_image(image_rgb)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Image not found at source_data/Augustus_Crop.jpg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0c37f7498942b5bdbf3c81508b6542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a63061e738416c85e76a623b04512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152388dd3bfa47b6ab13c3797fe19853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for red is IntegerRGB(red=255, green=0, blue=0)\n",
      "RGB value for red is 255 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eca3f33937c4576963b17e5d1b3015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"burgundy\" is not defined as a named color in css3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mon_apply_button_clicked\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_apply_button_clicked\u001b[39m(b):\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mhair_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_dropdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mhair_transform\u001b[1;34m(selected_color, image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhair_transform\u001b[39m(selected_color, image_path):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mset_hair_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m, in \u001b[0;36mset_hair_color\u001b[1;34m(color_name, input_image_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_image_cv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     13\u001b[0m   input_image_cv \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(input_image_cv, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2BGRA)\n\u001b[1;32m---> 15\u001b[0m rgb_value \u001b[38;5;241m=\u001b[39m \u001b[43mwebcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB value for\u001b[39m\u001b[38;5;124m'\u001b[39m, color_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, rgb_value)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB value for\u001b[39m\u001b[38;5;124m'\u001b[39m, color_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, rgb_value\u001b[38;5;241m.\u001b[39mred, rgb_value\u001b[38;5;241m.\u001b[39mgreen, rgb_value\u001b[38;5;241m.\u001b[39mblue)\n",
      "File \u001b[1;32mc:\\Users\\Sarja\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\webcolors\\_conversion.py:78\u001b[0m, in \u001b[0;36mname_to_rgb\u001b[1;34m(name, spec)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_to_rgb\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, spec: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m CSS3) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IntegerRGB:\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Convert a color name to a 3-:class:`tuple` of :class:`int` suitable for use in\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    an ``rgb()`` triplet specifying that color.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hex_to_rgb(\u001b[43mname_to_hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Sarja\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\webcolors\\_conversion.py:51\u001b[0m, in \u001b[0;36mname_to_hex\u001b[1;34m(name, spec)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hex_value \u001b[38;5;241m:=\u001b[39m color_map\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m.\u001b[39mlower()):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hex_value\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not defined as a named color in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \"burgundy\" is not defined as a named color in css3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd25abb7fb274739872602b7c8aaf31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2df037990f4cc8919fb88c5ab6b0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hair mask saved to: output/masks/hair_mask.png\n",
      "Hair mask generated at output/masks/hair_mask.png\n",
      "RGB value for magenta is IntegerRGB(red=255, green=0, blue=255)\n",
      "RGB value for magenta is 255 0 255\n"
     ]
    }
   ],
   "source": [
    "color_dropdown = widgets.Dropdown(\n",
    "  options=['blue', 'red', 'green', 'yellow', 'purple', 'pink', 'orange', 'brown', 'black', 'white','gray', 'cyan', 'magenta' ],\n",
    "  value='blue',\n",
    "  description='Color:',\n",
    ")\n",
    "\n",
    "# Create a text input for image path\n",
    "image_path_text = widgets.Text(\n",
    "  value='source_data/stefan_original_photo.jpg',\n",
    "  description='Image Path:',\n",
    ")\n",
    "\n",
    "# Create a button to apply the hair color\n",
    "apply_button = widgets.Button(\n",
    "    description='Apply Hair Color',\n",
    ")\n",
    "\n",
    "# Define the function to be called when the button is clicked\n",
    "def on_apply_button_clicked(b):\n",
    "    hair_transform(color_dropdown.value, image_path_text.value)\n",
    "\n",
    "apply_button.on_click(on_apply_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(color_dropdown, image_path_text, apply_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
